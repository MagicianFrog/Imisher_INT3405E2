{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import networkx as nx\n",
    "\n",
    "# --- CÀI ĐẶT THƯ VIỆN BỔ SUNG ---\n",
    "try:\n",
    "    import obonet\n",
    "except ImportError:\n",
    "    os.system('pip install obonet networkx')\n",
    "    import obonet\n",
    "\n",
    "# --- CẤU HÌNH ---\n",
    "CFG = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'batch_size': 128,      \n",
    "    'epochs': 25,           # Train kỹ hơn một chút vì dữ liệu xịn\n",
    "    'lr': 2e-4,             # Tốc độ học ổn định\n",
    "    'top_n_terms': 3000,    # 3000 nhãn\n",
    "    'input_dim': 1280,      # KÍCH THƯỚC QUAN TRỌNG CỦA ESM-2 650M\n",
    "    'data_path': '/kaggle/input/cafa-6-protein-function-prediction'\n",
    "}\n",
    "\n",
    "print(f\">>> RUNNING ON: {CFG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. LOAD DỮ LIỆU TỪ DATASET\n",
    "# ==========================================\n",
    "print(\"\\n[1/5] LOADING EMBEDDINGS...\")\n",
    "\n",
    "def find_npy_file(keyword):\n",
    "    paths = glob.glob(f'/kaggle/input/**/*{keyword}*.npy', recursive=True)\n",
    "    preferred_paths = [p for p in paths if '650M' in p]\n",
    "    if preferred_paths:\n",
    "        return max(preferred_paths, key=os.path.getsize)\n",
    "    elif paths:\n",
    "        print(f\"! Cảnh báo: Không thấy tên file chứa '650M', dùng file tìm được: {paths[0]}\")\n",
    "        return max(paths, key=os.path.getsize)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"LỖI: Không tìm thấy file npy nào chứa từ khóa '{keyword}' trong Input.\")\n",
    "\n",
    "try:\n",
    "    train_path = find_npy_file('train') \n",
    "    test_path = find_npy_file('test')  \n",
    "    \n",
    "    print(f\" -> Found Train File: {train_path}\")\n",
    "    print(f\" -> Found Test File:  {test_path}\")\n",
    "    \n",
    "    X_train_emb = np.load(train_path)\n",
    "    X_test_emb = np.load(test_path)\n",
    "    if X_train_emb.shape[1] != 1280:\n",
    "        print(f\"Dữ liệu input có kích thước {X_train_emb.shape[1]}\")\n",
    "        CFG['input_dim'] = X_train_emb.shape[1]\n",
    "    \n",
    "    print(f\" -> Loaded Shapes: Train={X_train_emb.shape}, Test={X_test_emb.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"LỖI LOAD FILE: {e}\")\n",
    "    raise\n",
    "def get_clean_ids(path):\n",
    "    ids = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                ids.append(line.strip()[1:].split('|')[1] if '|' in line else line.strip()[1:].split()[0])\n",
    "    return ids\n",
    "\n",
    "print(\" -> Loading Raw Metadata...\")\n",
    "train_ids = get_clean_ids(f\"{CFG['data_path']}/Train/train_sequences.fasta\")\n",
    "test_ids = get_clean_ids(f\"{CFG['data_path']}/Test/testsuperset.fasta\")\n",
    "\n",
    "train_terms = pd.read_csv(f\"{CFG['data_path']}/Train/train_terms.tsv\", sep='\\t')\n",
    "top_terms = train_terms['term'].value_counts().head(CFG['top_n_terms']).index.tolist()\n",
    "\n",
    "# Map Matrix Y\n",
    "train_id_map = {pid: i for i, pid in enumerate(train_ids)}\n",
    "valid_terms = train_terms[train_terms['EntryID'].isin(train_ids) & train_terms['term'].isin(top_terms)]\n",
    "Y_df = valid_terms.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "Y_df = (Y_df > 0).astype(int)\n",
    "\n",
    "# Đồng bộ\n",
    "common_ids = sorted(list(set(train_ids) & set(Y_df.index)))\n",
    "x_indices = [train_id_map[pid] for pid in common_ids]\n",
    "\n",
    "X_final = X_train_emb[x_indices]\n",
    "Y_final = Y_df.loc[common_ids].values\n",
    "target_names = Y_df.columns.tolist()\n",
    "\n",
    "# Dọn RAM\n",
    "del X_train_emb, train_terms, valid_terms, Y_df\n",
    "gc.collect()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_final, Y_final, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. MODEL DEFINITION (ENHANCED MLP)\n",
    "# ==========================================\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y) if y is not None else None\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None: return self.x[idx], self.y[idx]\n",
    "        return self.x[idx]\n",
    "\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.bn_input = nn.BatchNorm1d(input_dim)\n",
    "        self.layers = nn.Sequential(\n",
    "            # Input 1280 -> 1024\n",
    "            nn.Linear(input_dim, 1024), nn.BatchNorm1d(1024), nn.ReLU(), nn.Dropout(0.35),\n",
    "            # 1024 -> 1024 (Lớp sâu hơn cho model 650M)\n",
    "            nn.Linear(1024, 1024), nn.BatchNorm1d(1024), nn.ReLU(), nn.Dropout(0.35),\n",
    "            # Output\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(self.bn_input(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. TRAINING LOOP\n",
    "# ==========================================\n",
    "print(\"\\n[2/5] TRAINING MODEL...\")\n",
    "\n",
    "train_loader = DataLoader(ProteinDataset(X_train, y_train), batch_size=CFG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(ProteinDataset(X_val, y_val), batch_size=CFG['batch_size'])\n",
    "\n",
    "model = EnhancedMLP(CFG['input_dim'], len(target_names)).to(CFG['device'])\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # Dùng BCE chuẩn cho ổn định\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(CFG['epochs']):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(CFG['device']), y.to(CFG['device'])\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(CFG['device']), y.to(CFG['device'])\n",
    "            val_loss += criterion(model(x), y).item()\n",
    "            \n",
    "    avg_val = val_loss/len(val_loader)\n",
    "    print(f\"Epoch {epoch+1:02d} | Val Loss: {avg_val:.4f}\")\n",
    "    \n",
    "    scheduler.step(avg_val)\n",
    "    \n",
    "    if avg_val < best_loss:\n",
    "        best_loss = avg_val\n",
    "        # Lưu model\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            torch.save(model.module.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= 5:\n",
    "            print(\"Early Stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. PREDICTION & PROPAGATION\n",
    "# ==========================================\n",
    "print(\"\\n[3/5] PREDICTING...\")\n",
    "# Load lại model sạch để dự đoán\n",
    "model = EnhancedMLP(CFG['input_dim'], len(target_names)).to(CFG['device'])\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(ProteinDataset(X_test_emb), batch_size=256, shuffle=False)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(test_loader):\n",
    "        preds.append(torch.sigmoid(model(x.to(CFG['device']))).cpu().numpy())\n",
    "final_probs = np.vstack(preds)\n",
    "\n",
    "print(\"\\n[4/5] APPLYING ONTOLOGY PROPAGATION...\")\n",
    "try:\n",
    "    obo_path = f\"{CFG['data_path']}/Train/go-basic.obo\"\n",
    "    go_graph = obonet.read_obo(obo_path if os.path.exists(obo_path) else \"http://purl.obolibrary.org/obo/go/go-basic.obo\")\n",
    "    \n",
    "    term_to_idx = {t: i for i, t in enumerate(target_names)}\n",
    "    term_parents = {}\n",
    "    for term, data in go_graph.nodes(data=True):\n",
    "        if 'is_a' in data:\n",
    "            parents = [p for p in data['is_a'] if p in term_to_idx]\n",
    "            if parents: term_parents[term] = parents\n",
    "\n",
    "    for _ in range(2):\n",
    "        for child, parents in term_parents.items():\n",
    "            if child in term_to_idx:\n",
    "                c_idx = term_to_idx[child]\n",
    "                c_scores = final_probs[:, c_idx]\n",
    "                for p in parents:\n",
    "                    p_idx = term_to_idx[p]\n",
    "                    final_probs[:, p_idx] = np.maximum(final_probs[:, p_idx], c_scores)\n",
    "    print(\" -> Propagation Done.\")\n",
    "except Exception as e:\n",
    "    print(f\" -> Propagation Failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T13:58:28.947004Z",
     "iopub.status.busy": "2025-11-25T13:58:28.946758Z",
     "iopub.status.idle": "2025-11-25T14:00:00.119119Z",
     "shell.execute_reply": "2025-11-25T14:00:00.118490Z",
     "shell.execute_reply.started": "2025-11-25T13:58:28.946978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. WRITING SUBMISSION\n",
    "# ==========================================\n",
    "print(\"\\n[5/5] WRITING SUBMISSION...\")\n",
    "CHUNK = 20000\n",
    "THRESHOLD = 0.01 \n",
    "\n",
    "with open('submission.tsv', 'w') as f: pass\n",
    "\n",
    "count = 0\n",
    "with open('submission.tsv', 'a') as f:\n",
    "    for i in tqdm(range(0, len(test_ids), CHUNK)):\n",
    "        end = min(i+CHUNK, len(test_ids))\n",
    "        chunk_p = final_probs[i:end]\n",
    "        chunk_id = test_ids[i:end]\n",
    "        \n",
    "        rows, cols = np.where(chunk_p > THRESHOLD)\n",
    "        \n",
    "        if len(rows) > 0:\n",
    "            df = pd.DataFrame({\n",
    "                'id': np.array(chunk_id)[rows],\n",
    "                'term': np.array(target_names)[cols],\n",
    "                'score': chunk_p[rows,cols]\n",
    "            })\n",
    "            df['score'] = df['score'].map('{:.3f}'.format)\n",
    "            df.to_csv(f, header=False, index=False, sep='\\t')\n",
    "            count += len(df)\n",
    "\n",
    "print(f\"\\n>>> DONE! Submission file created with {count} rows.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 3167603,
     "sourceId": 5499219,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3197305,
     "sourceId": 5549164,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3225525,
     "sourceId": 5607816,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3327296,
     "sourceId": 5792099,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8869432,
     "sourceId": 13919221,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 285101544,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 285103218,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
