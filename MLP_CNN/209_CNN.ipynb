{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import networkx as nx\n",
    "\n",
    "# --- CÀI ĐẶT ---\n",
    "try:\n",
    "    import obonet\n",
    "except ImportError:\n",
    "    os.system('pip install obonet networkx')\n",
    "    import obonet\n",
    "\n",
    "# --- CẤU HÌNH ---\n",
    "CFG = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'batch_size': 128,      \n",
    "    'epochs': 25,           \n",
    "    'lr': 1e-3,           \n",
    "    'top_n_terms': 3000,    \n",
    "    'input_dim': 1280,    \n",
    "    'data_path': '/kaggle/input/cafa-6-protein-function-prediction'\n",
    "}\n",
    "\n",
    "print(f\">>> TRAINING 1D-CNN ON: {CFG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. LOAD DATA (AUTO-DETECT)\n",
    "# ==========================================\n",
    "print(\"\\n[1/5] LOADING DATA...\")\n",
    "def find_npy_file(keyword):\n",
    "    paths = glob.glob(f'/kaggle/input/**/*{keyword}*.npy', recursive=True)\n",
    "    preferred = [p for p in paths if '650M' in p]\n",
    "    return max(preferred, key=os.path.getsize) if preferred else max(paths, key=os.path.getsize)\n",
    "\n",
    "try:\n",
    "    X_train_emb = np.load(find_npy_file('train'))\n",
    "    X_test_emb = np.load(find_npy_file('test'))\n",
    "    print(f\" -> Loaded: Train={X_train_emb.shape}, Test={X_test_emb.shape}\")\n",
    "except:\n",
    "    raise FileNotFoundError(\"Chưa Add Dataset chứa file .npy!\")\n",
    "\n",
    "# Metadata\n",
    "def get_clean_ids(path):\n",
    "    ids = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                ids.append(line.strip().split('|')[1] if '|' in line else line.strip()[1:].split()[0])\n",
    "    return ids\n",
    "\n",
    "train_ids = get_clean_ids(f\"{CFG['data_path']}/Train/train_sequences.fasta\")\n",
    "test_ids = get_clean_ids(f\"{CFG['data_path']}/Test/testsuperset.fasta\")\n",
    "train_terms = pd.read_csv(f\"{CFG['data_path']}/Train/train_terms.tsv\", sep='\\t')\n",
    "top_terms = train_terms['term'].value_counts().head(CFG['top_n_terms']).index.tolist()\n",
    "\n",
    "# Mapping\n",
    "train_id_map = {pid: i for i, pid in enumerate(train_ids)}\n",
    "valid_terms = train_terms[train_terms['EntryID'].isin(train_ids) & train_terms['term'].isin(top_terms)]\n",
    "Y_df = valid_terms.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "Y_df = (Y_df > 0).astype(int)\n",
    "\n",
    "common_ids = sorted(list(set(train_ids) & set(Y_df.index)))\n",
    "x_indices = [train_id_map[pid] for pid in common_ids]\n",
    "\n",
    "X_final = X_train_emb[x_indices]\n",
    "Y_final = Y_df.loc[common_ids].values\n",
    "target_names = Y_df.columns.tolist()\n",
    "\n",
    "del X_train_emb, train_terms, valid_terms, Y_df\n",
    "gc.collect()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_final, Y_final, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. MODEL: 1D-CNN\n",
    "# ==========================================\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y) if y is not None else None\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None: return self.x[idx], self.y[idx]\n",
    "        return self.x[idx]\n",
    "\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        return self.relu(out)\n",
    "\n",
    "class ProteinCNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "            ResidualBlock1D(32, 64),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            ResidualBlock1D(64, 128),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            ResidualBlock1D(128, 256),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. TRAINING\n",
    "# ==========================================\n",
    "print(\"\\n[2/5] TRAINING 1D-CNN...\")\n",
    "\n",
    "train_loader = DataLoader(ProteinDataset(X_train, y_train), batch_size=CFG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(ProteinDataset(X_val, y_val), batch_size=CFG['batch_size'])\n",
    "\n",
    "model = ProteinCNN(CFG['input_dim'], len(target_names)).to(CFG['device'])\n",
    "if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(CFG['epochs']):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(CFG['device']), y.to(CFG['device'])\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(CFG['device']), y.to(CFG['device'])\n",
    "            val_loss += criterion(model(x), y).item()\n",
    "            \n",
    "    avg_val = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1:02d} | Val Loss: {avg_val:.4f}\")\n",
    "    \n",
    "    scheduler.step(avg_val)\n",
    "    if avg_val < best_loss:\n",
    "        best_loss = avg_val\n",
    "        # Lưu model\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            torch.save(model.module.state_dict(), 'best_cnn_model.pth')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= 5:\n",
    "            print(\"Early Stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. PREDICT & PROPAGATE\n",
    "# ==========================================\n",
    "print(\"\\n[3/5] PREDICTING...\")\n",
    "model = ProteinCNN(CFG['input_dim'], len(target_names)).to(CFG['device'])\n",
    "model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(ProteinDataset(X_test_emb), batch_size=256, shuffle=False)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(test_loader):\n",
    "        preds.append(torch.sigmoid(model(x.to(CFG['device']))).cpu().numpy())\n",
    "final_probs = np.vstack(preds)\n",
    "\n",
    "print(\"\\n[4/5] APPLYING ONTOLOGY PROPAGATION...\")\n",
    "try:\n",
    "    obo_path = f\"{CFG['data_path']}/Train/go-basic.obo\"\n",
    "    go_path = obo_path if os.path.exists(obo_path) else \"http://purl.obolibrary.org/obo/go/go-basic.obo\"\n",
    "    go_graph = obonet.read_obo(go_path)\n",
    "    \n",
    "    term_to_idx = {t: i for i, t in enumerate(target_names)}\n",
    "    term_parents = {}\n",
    "    for term, data in go_graph.nodes(data=True):\n",
    "        if 'is_a' in data:\n",
    "            parents = [p for p in data['is_a'] if p in term_to_idx]\n",
    "            if parents: term_parents[term] = parents\n",
    "\n",
    "    for _ in range(2):\n",
    "        for child, parents in term_parents.items():\n",
    "            if child in term_to_idx:\n",
    "                c_idx = term_to_idx[child]\n",
    "                c_scores = final_probs[:, c_idx]\n",
    "                for p in parents:\n",
    "                    p_idx = term_to_idx[p]\n",
    "                    final_probs[:, p_idx] = np.maximum(final_probs[:, p_idx], c_scores)\n",
    "    print(\" -> Propagation Done.\")\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T13:58:28.947004Z",
     "iopub.status.busy": "2025-11-25T13:58:28.946758Z",
     "iopub.status.idle": "2025-11-25T14:00:00.119119Z",
     "shell.execute_reply": "2025-11-25T14:00:00.118490Z",
     "shell.execute_reply.started": "2025-11-25T13:58:28.946978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. WRITE SUBMISSION \n",
    "# ==========================================\n",
    "print(\"\\n[5/5] WRITING SUBMISSION CNN...\")\n",
    "with open('submission.tsv', 'w') as f: pass\n",
    "with open('submission.tsv', 'a') as f:\n",
    "    CHUNK = 20000\n",
    "    for i in tqdm(range(0, len(test_ids), CHUNK)):\n",
    "        end = min(i+CHUNK, len(test_ids))\n",
    "        chunk_p = final_probs[i:end]\n",
    "        chunk_id = test_ids[i:end]\n",
    "        rows, cols = np.where(chunk_p > 0.01)\n",
    "        if len(rows)>0:\n",
    "            df = pd.DataFrame({\n",
    "                'id': np.array(chunk_id)[rows],\n",
    "                'term': np.array(target_names)[cols],\n",
    "                'score': chunk_p[rows,cols]\n",
    "            })\n",
    "            df['score'] = df['score'].map('{:.3f}'.format)\n",
    "            df.to_csv(f, header=False, index=False, sep='\\t')\n",
    "\n",
    "print(\"\\nALL DONE\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 3167603,
     "sourceId": 5499219,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3197305,
     "sourceId": 5549164,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3225525,
     "sourceId": 5607816,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3327296,
     "sourceId": 5792099,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8869432,
     "sourceId": 13919221,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 285101544,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 285103218,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
